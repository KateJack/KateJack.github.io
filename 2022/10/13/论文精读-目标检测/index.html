<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  
  <title>论文精读-目标检测 [ 刘盼的技术博客 ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/iLiKE.css">
    
  
  
  
  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
    <script id="leancloud">
      AV.init({
          appId: "6E5zTbTljdUbVW2WkXPsXGJk-gzGzoHsz",
          appKey: "0vsyDKfNpeSECAI70J794ugv"
      });
    </script>

<meta name="generator" content="Hexo 6.3.0"></head>
<body>
    <div class="header">
        <div class="container">
    <div class="menu">
      <div class="menu-left">
        <a href="/">
          <img src="/favicon.ico"></img>
        </a>
      </div>
      <div class="menu-right">
        
          
          
          
          
          
          
          <a href="/">Home</a>
        
          
          
          
          
          
          
          <a href="/archives">Archives</a>
        
      </div>
    </div>
</div>
    </div>
    <div class="container">
        <h1 class="post-title">论文精读-目标检测</h1>
<article class="post markdown-style">
  <h1 id="Deep-Learning-for-Feneric-Object-Detection-A-Survey"><a href="#Deep-Learning-for-Feneric-Object-Detection-A-Survey" class="headerlink" title="Deep Learning for Feneric Object Detection: A Survey"></a>Deep Learning for Feneric Object Detection: A Survey</h1><h1 id="1、基于区域的两阶段框架（Two-Stage"><a href="#1、基于区域的两阶段框架（Two-Stage" class="headerlink" title="1、基于区域的两阶段框架（Two Stage)"></a>1、基于区域的两阶段框架（Two Stage)</h1><ul>
<li><p>RCNN</p>
<p>Girshick 将<strong>AlexNet</strong>以及<strong>候选区域选择性搜索</strong>（selective search）集成到一起，训练出了RCNN框架，具体步骤如下：</p>
<p>（1）计算建议窗口：通过选择性搜索获得可能包含目标的候选区域（region proposal）；</p>
<p>（2）微调CNN模型：从图像中裁剪并变形成相同大小的候选区域，当做预训练的CNN（例如使用大型数据集ImageNet）模型的输入。在此阶段，所有IOU的候选区域被定义为改正确标注框类的正例，其余的定义为反例</p>
<p>（3)训练特定类的SVM分类器：使用CNN提取的固定长度特征训练一组特定类的线性SVM分类器，取代了通过微调学习的softmax分类器。对于训练SVM分类器，将正样本定义为每个类的正确标注框。与类的所有正确标注框重叠小于0.3的候选区域对该类是否定的。</p>
<p>（4）训练特定类的边界框（回归任务）：针对每个具有CNN特征的目标类学习边界框回归</p>
<ul>
<li>缺点：</li>
</ul>
<p>（1）训练是个多阶段的流水任务，各阶段相互独立，训练缓慢且难以优化；</p>
<p>（2）训练SVM和边界框回归器在时间和空间上代价太大，需要从每个图像的每个候选目标中提取CNN特征，这在大规模检测和深度网络中的代价是巨大的</p>
<p>（3）测试速度很慢，每个测试图像都在根据候选目标独立的提取CNN特征，没有采用共享计算。</p>
</li>
<li><p>SPPNet<br>  在测试过程中，CNN特征提取是RCNN检测管道的主要瓶颈，这需要从每张图像的数千个扭曲候选区域中提取CNN特征。因此，SPPNet先对图像做一次卷积运算然后在特征图上提取特征，并且在卷积神经网络CNN之后增加了图像空间金字塔池化（Spatial Pyramid Pooling，SPP）的结构，这样就可以根据图像的特征将图像当中的目标区域进行分类。SPPNet和RCNN的区别如下图所示：<br>  <img src="https://images-pl.oss-cn-hangzhou.aliyuncs.com/images/a578d6c331594dfc9b059f0b66f43fc1.png" alt="在这里插入图片描述"><br>  空间金字塔池化结构<br>  <img src="https://images-pl.oss-cn-hangzhou.aliyuncs.com/images/9d58e261f5e641149686acc4f64ed657.png" alt="在这里插入图片描述"><br>  空间金字塔池化层的结构如上所示，Image经过一次卷积之后会得到256个特征图，也就是上面图中最下面的一连串黑色堆叠图，这是整个卷积神经网络的第五层吗，因此称为conv5。然后我们使用SSP结构对这256个特征图进行处理，将这256张特征图分别进行1<em>1，2</em>2，4<em>4的最大池化，也就是分别选取这256个特征图当中的最大值，然后最后的输出也是256个每一层特征图的最大值。比如我们做空间金字塔池化最右边的那个1</em>1池化，计算机仅仅会选取这256个特征图当中最大的值作为输出的结果作为图像的语义特征，这也就是最大池化，2<em>2的最大池化同理，但我们会做完2</em>2的最大池化之后会得到4个数值，每一个数值都代表这图像某一区域的特征。因此最后我们会得到21（21&#x3D;4<em>4+1</em>1+2<em>2）</em>256&#x3D;5376个数值作为图像高度抽象的特征。之后将这5376个特征送入全连接神经网络（fc6和fc7，一共两层全连接神经网络，这个完全看研究员自己的喜好来设定了，可以没有，也可以多层），最后用SVM分类器输出bounding box的x,y,w,h以及每一个bounding box的图象分类的结果。<br>  SPPNet流程步骤：<br>  （1）使用EdgeBoxes算法生成候选区域<br>  （2）使用CNN提取全图特征<br>  （3）将候选区域特征输入到SVM分类器，判别输入类别<br>  （4）以回归的方式精修候选框<br>  SPP-Net 是对 input image 一次性 conv 操作，相对于 R-CNN 大大提升了效率。SPP-Net 的亮点 就是 SPP 层，它可以让不同输入的 image，最后都得到相同的 feature maps，让网络多尺度训练和测试更便捷。SPP-Net 虽然解决了 R-CNN 许多大量冗余的问题，但是，由于还沿用<br>  了 R-CNN 的结构 和 SVM 分类器，只能单独进行 bbox regression。</p>
</li>
<li><pre><code>Fast RCNN
  为了解决 R-CNN 和 SPP-Net 2k 个左右 bounding boxes 带来的重复计算问题，于是 R-CNN 的作者研发出了 Fast R-CNN。主要思想：
</code></pre>
<p>  （1）使用一个简化的SPP层<br>  使用RoI(Region of Interest) pooling层，操作与SPP类似<br>  （2）训练和测试不再分多步<br>  不再需要额外的硬盘来存储中间层的特征，梯度能够通过RoI pooling层直线传播。此外，分类和回归用multitask的方式一起进行<br>  （3）SVD<br>  使用SVD分解全连接层的参数矩阵，压缩为两个规模小很多的全连接层。<br>  <strong>RoI pooling</strong>的具体操作：<br>  1）根据输入image，将RoI映射到feature map对应位置<br>  2）将映射后的感兴趣区域划分为不同大小（也可以有相同大小）的sections，sections数量与输出的维度相同。<br>  3）对每个sections进行max pooling操作。<br>  这样就可以从不同大小的方框得到固定大小相应的feature map。值得一提的是，输出的feature map的大小不取决于RoI和卷积feature map的大小。RoI pooling最大的好处就在于极大地提高了处理速度。<br>  <strong>Fast RCNN的主要步骤：</strong><br>  （1）region proposal<br>  通过selective search等方法从原始图像中提取region proposal bounding boxes<br>  （2）特征提取<br>  将整张图输入到CNN网络中，得到相关的feature maps<br>  （3）映射<br>  将bounding boxes一一映射到最后的feature maps<br>  （4）区域归一化<br>  针对feature maps上的每个region proposal bounding boxes进行RoI pooling操作，得到固定大小的feature maps<br>  （5）分类与回归<br>  通过两个全连接层，分别用softmax多分类的目标检测（以前是用SVM），用回归模型进行边框位置与大小的微调。<br>  <img src="https://images-pl.oss-cn-hangzhou.aliyuncs.com/images/7954a47301cf4cd0b9b11494690f6dcc.png" alt="在这里插入图片描述"><br>  SPP-Net 是个好方法，R-CNN 进阶成 Fast R-CNN 就是在 R-CNN 的基础上采纳了 SPP-Net 方法，对 R-CNN 做了改进，使得性能进一步提高。Fast R-CNN 其实就是用 ROI pooling 替换掉<br>  SPP layer，用 multi-task loss 替换掉 SVM 的 SPP-Net。由于 Fast R-CNN 是对 input image 是整张图像进行 CNN 操作，并使用了 ROI pooling 所以，它在效率上，比 R-CNN 和 SPP-Net 都要快，并且没有 R-CNN crop 或 warp 导致的误差。再者，Fast R-CNN 使用了 softmax 替代SVM， 使得模型训练和测试的分类效率更好了，而且，效果也明显提升。对于有限的数据使用 SVM 的效果会好点，但是，当用户拥有海量数据去做模型的时候，SVM 的处境就非常尴尬了。因为，它的计算方式是 把目标一个个分开来计算的，而不是像 softmax 一样，是<br>  一起计算的；这也导致在分类类别比较多的时候，SVM 的模型会相对比较大。</p>
</li>
<li><p>Faster RCNN<br>  RCNN的性能瓶颈主要都在获取候选区域上：<br>  （1）RoI的获取太麻烦（selective search）且正确率不高<br>  （2）RoI的特征提取太耗时（先切图片，然后CNN提取特征，导致数据无法共享），且中间涉及大量的硬盘读写，数据要在硬盘、内存和现存三者之间来回倒腾。<br>  Fast RCNN解决了第二个问题，为解决第一个问题，Ren提出直接利用建议窗口网络RPN（Region Proposal Network）来计算bounding boxes。RPN以一张任意大小的图像为输入，输出一批矩形候选区域，每个区域对应一个目标分类和位置信息。<br>  RPN的输出是一组boxes&#x2F;proposal，分类器和回归器将对这些boxes&#x2F;proposal进行检查，最终检查对象的出现情况。更精细的说，RPN网络预测了一个anchor是前景还是背景，并对anchor进行了细分。RPN的具体做法：<br>  （1）在最后一个卷积层的后面加入RPN网络，在feature maps上进行滑动窗口。<br>  （2）建一个神经网络用于物体分类+框位置的回归<br>  （3）滑动窗口的位置，提供了物体的大体位置信息<br>  （4）框的回归提供了框更精确的位置<br>  （5）通过训练得到bounding boxes region<br>  Faster RCNN的主要步骤：<br>  （1）特征提取<br>  将整张图像输入CNN网络中，利用CNN得到图像的feature maps<br>  （2）region proposal<br>  将上面CNN得到的feature maps输入到RPN网络中，利用K个不同的anchor boxes进行滑动窗口卷积操作，通过RPN网络的分类与回归进行提名，每张图像大约生成300个建议窗口。<br>  （3）RoI pooling<br>  将生成的建议窗口一一映射到上面CNN得到的feature maps上，通过RoI pooling使每个RoI生成固定尺寸的feature map<br>  （4）分类与回归<br>  利用softmax loss（探测分类概率）和smooth loss（探测边框回归）对分类概率和边框回归的联合训练。<br>  <img src="https://images-pl.oss-cn-hangzhou.aliyuncs.com/images/618cd6e6c002498abbdb713d91c0cc0d.png" alt="在这里插入图片描述"><br>  Faster R-CNN 抛弃了 selective search，引入了 RPN 网络。在 RPN 中，当 RPN 与目标网络共享最大计算量时，生成 region proposals 区域提名 的世界开销比 selective search 选择性搜索 小得多。简单地说，RPN 对区域框 (或理解为 anchor 框) 进行排序，并提出最有可能包含对象的区域框。这使得 region proposals、分类、回归 一起共享卷积特征，从而得到了进一步加速。</p>
</li>
<li><p>RFCN（Region Fully Convolutional Network）<br>  虽然Faster RCNN已经比Fast RCNN快了一个数量级，但每个RoI（每个图像几百个RoI）仍然需要应用区域子网络进行计算。所以Dai提出了RFCN检测器，该监测器是完全卷积的（没有FC层），几乎所有计算都在整个图像上共享。<br>  RFCN仅在RoI子网络中不同于Faster RCNN。在Faster RCNN中，RoI池化层后的计算不能共享，因此Dai et al.提出使用所有卷积层构建共享RoI子网络，通过使用一组专门的卷积层作为FCN输出，构造了一组位置敏感的分数图，在其上增加了位置敏感的RoI池层。</p>
</li>
<li><p>Mask RCNN<br>  Mask RCNN采用相同的两级流水线，具有相同的第一级 (RPN)，但是在第二阶段，与预测类和边框回归并行，Mask RCNN添加了一个分支，该分支为每个RoI输出二进制Mask Prediction。新分支是位于CNN特征地图之上的全卷积网络 (FCN)。为了避免原始RoI池 (RoIPool) 层引起的不对齐，提出了RoIAlign层以保留像素级空间对应关系。借助骨干网ResNeXt101-FPN，Mask RCNN在COCO目标实例分割和边界框对象检测方面取得了最好的结果。它训练简单，概括得很好，并且只为Faster RCNN增加了很小的开销，以5 FPS的速度运行。</p>
</li>
</ul>
<h1 id="2、统一框架（One-Stage）"><a href="#2、统一框架（One-Stage）" class="headerlink" title="2、统一框架（One Stage）"></a>2、统一框架（One Stage）</h1><ul>
<li>DetectorNet<br>  DetectorNet将目标检测公式化为目标边界框掩码的回归问题。他们使用AlexNet，并将softmax分类器层替换为回归层。给定一个图像窗口，他们使用一个网络来预测粗糙网格上的前景像素，以及四个其他网络来预测目标的顶部，底部，左侧和右侧。然后，分组过程将预测的掩码转换为检测到的边界框。网络需要按目标类型和掩码类型进行训练，并且不扩展到多个类。DetectorNet必须获取图像的许多crops，并在每个crops上的每个部分运行多个网络，从而使其变慢。</li>
<li>OverFeat<br>  OverFeat通过网络中的全卷积层（即“特征提取器”）的单个前向传递来执行目标检测。测试时的关键步骤如下：<br>  （1）在多尺度图像上通过滑动窗口方式执行目标分类来生成目标候选对象。为了使滑动窗<br>      口的方法在计算上高效，OverFeat将网络转换为全卷积网络，通过将完全连接的层视<br>      为大小为1 × 1的核的卷积，获取任何大小的输入。OverFeat利用多尺度特征，通过<br>      将原始图像的多达六个放大尺度传递通过网络来提高整体性能，从而导致评估的上下<br>      文视图的数量显著增加。对于每个多尺度输入，分类器输出一个预测网格 (类和置信<br>      度)<br>  （2）通过偏移最大池化增加预测数量。为了提高分辨率，OverFeat在最后一个卷积层之后<br>      应用偏移最大池化，即在每个偏移处执行自采样操作，产生更多用于投票的视图，在<br>      保持效率的同时增加鲁棒性。<br>  （3）边界框回归识别对象后，将应用单个边界框回归器。分类器和回归器共享相同的特征<br>  提取(卷积) 层，仅在计算分类网络后需要重新计算FC层。<br>  （4）组合预测OverFeat使用贪婪的合并策略来组合所有位置和比例的各个边界框预测。</li>
<li>YOLO<br>  由于候选区域生成阶段已完全删除，因此YOLO使用一小部分候选区域直接进行预测。与基于区域的方法 (例如，Faster RCNN) 不同，该方法基于来自本地区域的特征进行检测，YOLO全局使用来自整个图像的特征。特别是，YOLO将图像划分为S×S的网格，每个网格都预测 C类概率、边界框位置B以及置信度得分。</li>
<li>SSD（Single Shot Detector）<br>  像YOLO一样，SSD会预测固定数量的边界框和分数，然后是NMS步骤以产生最终检测。SSD中的CNN网络是完全卷积的，其早期层基于标准架构（例如，VGG），随后是几个辅助卷积层，大小逐渐减小。最后一层中的信息可能在空间上过于粗糙，以至于无法精确定位，因此SSD通过对多个卷积特征映射进行操作来在多个尺度上执行检测，每个卷积特征映射都预<br>  测了具有适当大小的边界框的类别分数和框偏移。<br>  NMS（Non-Maximum suppression）非极大值抑制，是目标检测算法中一个必要的后处理过程，目的是消除同一个物体上的冗余预测框。<br>  NMS算法的主要思想是：先对网络预测出的所有边界框按照分数由高到低排序，然后选取分数最高的预测框作为target，分别计算target与其余剩下的预测框的重叠程度（用IoU来衡量），若重叠程度大于某一预先设定的阈值，则认为该预测框与target是同时负责预测同一个物体的，所以将该边界框删除，否则予以保留。接着在未被删除的预测框中选择分数最高的预测框作为新target，重复以上过程，直至判断出所有的框是否应该删除。</li>
</ul>

</article>

    <div class="pagenator post-pagenator">
    
    
        <a class="extend prev post-prev" href="/2022/12/25/%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/">prev</a>
    

    
    <p>last update time 2023-08-24</p>
    
    
        <a class="extend next post-next" href="/2022/08/10/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E9%81%8D%E5%8E%86%E7%AE%97%E6%B3%95/">next</a>
    
    </div>


    </div>
    <div class="footer">
        <div class="container">
    <div class="social">
	<ul class="social-list">
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
	</ul>
</div>
    <div class="copyright">
        <span>
            
            
            
                © 刘盼 2022 - 2023
            
        </span>
    </div>
    <div class="power">
        <span>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/CaiChenghan/iLiKE">iLiKE Theme</a>
        </span>
    </div>
    <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
    <!--page counter part-->
<script>
function addCount (Counter) {
    url=$('.article-date').attr('href').trim();
    title = $('.article-title').text().trim();
    var query = new AV.Query(Counter);
    //use url as unique idnetfication
    query.equalTo("url",url);
    query.find({
        success: function(results) {
            if (results.length>0) {
                var counter=results[0];
                counter.fetchWhenSave(true); //get recent result
                counter.increment("time");
                counter.save();
            } else {
                var newcounter=new Counter();
                newcounter.set("title",title);
                newcounter.set("url",url);
                newcounter.set("time",1);
                newcounter.save(null,{
                    success: function(newcounter) {
                        //alert('New object created');
                    }, error: function(newcounter,error) {
                        alert('Failed to create');
                    }
                })
            }
        },
        error: function(error) {
            //find null is not a error
            alert('Error:'+error.code+" "+error.message);
        }
    });
}
$(function() {
    var Counter=AV.Object.extend("Counter");
    //only increse visit counting when intering a page
    if ($('.article-title').length == 1) {
       addCount(Counter);
    }
    var query=new AV.Query(Counter);
    query.descending("time");
    // the sum of popular posts
    query.limit(10); 
    query.find({
        success: function(results) {
                for(var i=0;i<results.length;i++) {
                    var counter=results[i];
                    title=counter.get("title");
                    url=counter.get("url");
                    time=counter.get("time");
                    // add to the popularlist widget
                    showcontent=title+" ("+time+")";
                    //notice the "" in href
                    $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                }
            },
        error: function(error) {
            alert("Error:"+error.code+" "+error.message);
        }
    });
});
</script>
</div>
    </div>
</body>
</html>
